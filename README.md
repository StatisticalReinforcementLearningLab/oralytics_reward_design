# Code for "Reward Design For An Online Reinforcement Learning Algorithm For Supporting Oral Self-Care‚Äù
---

## Organization
The package is organized in the following way:

### Simulation Environment
* Fitting parameters for the simualtion environment base model: [fitting_simulation_base_model.py](https://github.com/StatisticalReinforcementLearningLab/oralytics_reward_design/blob/main/code/fitting_simulation_base_model.py)
* Forming the prior for the RL algorithm: [forming_prior_using_robas_2.py](https://github.com/StatisticalReinforcementLearningLab/oralytics_reward_design/blob/main/code/forming_prior_using_robas_2.py)
* Simulation Envrionment Code: [simulation_environment.py](https://github.com/StatisticalReinforcementLearningLab/oralytics_reward_design/blob/main/code/simulation_environment.py)

### RL Algorithm
* RL Algorithm Code: [rl_algorithm.py](https://github.com/StatisticalReinforcementLearningLab/oralytics_reward_design/blob/main/code/rl_algorithm.py)

### Experiments
* Main Experiment Function: [rl_experiments.py](https://github.com/StatisticalReinforcementLearningLab/oralytics_reward_design/blob/main/code/rl_experiments.py)
* Code for parallelization and calling experiment function: [run_rl_experiment.py](https://github.com/StatisticalReinforcementLearningLab/oralytics_reward_design/blob/main/code/run_rl_experiment.py)

### Metrics and Figures
* Computing metrics and plotting figures: [metric_computations.py](https://github.com/StatisticalReinforcementLearningLab/oralytics_reward_design/blob/main/code/metric_computations.py) and
[plot_figures.py](https://github.com/StatisticalReinforcementLearningLab/oralytics_reward_design/blob/main/code/plot_figures.py)

